---
title: "Bugs statistic"
author: "Alexey Popov"
date: '18 июня 2018 г '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(ggplot2)
require(moments)
require(nortest)
require(stats)
        
require(tidyverse)

```


## Баги и статистика


# Что мы хотим узнать?

Мы работает используя Agile/Scrum подход с недельными спринтами и оценкой задач в идеальных часах по шкале (1,2,4,8,16,24,40,..). На спринты редко попадают задачи в 24 часа, так как опытным путем установлено, что 24 идеальных часа это несколько больше 48 астрономических, то есть такая задача априори имеет хорошие шансы быть недоделанной - такие задачи просто не влазят в недельные спринты. Однако бывает и так: такие задачи на спринт все таки берутся и, как ни странно, выполняются. Не всегда, но достаточно часто. Почему? Потому что 24 - это усредненная  оценка содержащая ошибку, то есть ее можно рассматривать как вероятностостную величину. Внимание! Далее везде используются оценки в идеальных часах, а не астрономических. Даже если происходит переоценка тикета после выполнения, делается это в идеальных часах. 

Что происходит, если поддеркжка приносит на спринт баг (воспроизводимый и оформленный, с подготоваленной средой, проанализированными логами, развернутыми дампами и т.д.)? Есть два варинат. Вариант первый: баг простой и понятный (ошибка в орфографии, не в том формате выводиться поле в отчет и т.д.) и тогда разрабочики сразу говорят - 1 час, или 2 часа, или "ну поставь 4, там надо повозиться..". Вариант второй: "а хрен его знает, надо посмотреть..." И на это "посмотреть" может уйти несколько часов, а то и дней. В этом случае мы ставим 4 часа (иногда, если сразу понятно что работы будет много - 8) и рассматриваем эту задачу как спайк: разразботчик должен изучить вопрос и а) исправить баг если трудозатраты в сумме меньше 4(8) часов или б) описать суть проблемы изнутри и предложить решение, предложить новую оценку и отложить фиксацию до следующего спринта. Однако на практике часто бывает так, что на исследование может уйти сильно больше 4(8) часов, зато исправление оказывается простым или относительно простым, и откладывать его еще на неделю как-то глупо. В этой ситуации исправление производится сразу, а задача переоценивается. Очевидно, что такие задачи будут "подрастать" по ходу спринта, и это не хорошо. С другой стороны, если баг казался сложным (4-х часовым), а исправили его за час, то есть происходит компенсация увеличение трудозатрат на сложные баги за счет снижение трудозатрат на простые. 
Конечно бывает и так, что баг, казавшийся простым и оцененный, скажем, в 1 час, на деле оказался сложнее и был исправлен, скажем, за 4 идеальных часа. К сожалению, на данный момент нет возможности получить априорную оценку по всем багам и сравнить, но по ощущению такое случается достаточно редко. 

Исходная оценка в 4 часа для сложных багов была выбрана интуитивно 2 года назад, также как и правила работы с бажными тикетами. Это было примерно спустя полгода после того как мы перешли на scrum. К текущему моменту накопилось уже достаточно приличная статистика по фиксации бажных тикетов. Поэтому первая группа вопросов ставиться так: правильно ли была выбрана исходная оценка в 4 часа? Происходит ли компенсация на практике? Правильно ли вообще мы работаем с бажными тикетами, не ли здесь какой-то систематической проблемы? 

Вторая группа вопросов касается свойств распределения случаной величины (то есть оценки трудозатрат на исправление бага) и происходит из моего спора с одиним известным бизнес-тренером. Вкратце суть спора такова. Я задал вопрос: что он может порекомендовать (какие инструменты, методики) для планирования в том случае, когда априори трудозатраты на задачу не известны и по определению не могут быть оценены до начала работ. Ответ тренера был примерно таков: трудозатраты можно рассматривать как вероятностную вуличину (тут я согласен) распределенную нормально (а вот тут я не был согласен) и просто брать среднюю данного распределения. Простые задачи будут компенсировать сложные и все будет хорошо (тут я тоже не согласился, еще не имея конкретных цифр на руках, но помня, что происходит на спринтах). Поэтому вторая группа вопросов ставиться так: является ли распределение трудозатрат на исправление багов нормальным? Можно ли в данном случае использовать свойства нормального распредления (критерий 3 сигмы, оценку вероятности редких событией и т.д.)?

# А можно ли так делать? 

Внимательный читатель может возразить, мол вы работаете не с самой случаной величиной, а с усреднением случаной величины по неравномерным интервалам. Да, это так. Но на самом деле это нам не мешает. Скорее наоборот, данная операция может приблизить наше распределение к нормальному, но вряд ли отдалит. (тут надо бы вставить немножко теории, но сейчас на это нет времени). Но давайте для верности проведем следственный эксперимент (это и быстрее, и убедительнее). Смоделируем выборку (нормально распределенную, с заданным средним и дисперсией), усредним по неравномерным интервалам и посмотрим, останется ли наше распределение нормально распределенным. 

Сфорируем выборку сопоставимую по размерам с реальной, возьмем близкие среденее и дисперсию.
```{r}
z <- rnorm(1000, mean = 4, sd = 4)
mean(z)
sd(z)
```
Как проверить, является ли данное распределение нормальным? Для наглядности построим гистограмму, посчитаем асимметрию и эксцесс. 
```{r}
ggplot() + geom_histogram(aes(x = z), fill = "gray70", colour = "gray10") +
  scale_x_continuous(breaks=seq(-15,15,by=1)) +
  theme_bw()

skewness(z)
kurtosis(z)
```
Как можно видеть - все прекрасно. Асимметрия нулевая, эксцесс ~3.  

Проведем тестированием используя критерия согласия Хи-квадрат (критерий Пирсона). Используем готовый тест и дополнительно посчитаем критерий согласия вручную. 
```{r}
# теоретическая плотность вероятности
chi_c <- c(pnorm(0.2, mean = 4, sd = 4),
           pnorm(0.75, mean = 4, sd = 4) - pnorm(0.2, mean = 4, sd = 4),
           pnorm(1.5, mean = 4, sd = 4) - pnorm(0.75, mean = 4, sd = 4),
           pnorm(3, mean = 4, sd = 4) - pnorm(1.5, mean = 4, sd = 4),
           pnorm(6, mean = 4, sd = 4) - pnorm(3, mean = 4, sd = 4),
           pnorm(12, mean = 4, sd = 4) - pnorm(6, mean = 4, sd = 4),
           pnorm(20, mean = 4, sd = 4) - pnorm(12, mean = 4, sd = 4),
           1 - pnorm(20, mean = 4, sd = 4))

# плотность распределения по модельным данным (не нормированная)
chi_o <- c(sum((z<=0.25)), 
             sum((z>0.25) & (z<=0.75)), 
             sum((z>0.75) & (z<=1.5)), 
             sum((z>1.5) & (z<=3)), 
             sum((z>3) & (z<=6)), 
             sum((z>6) & (z<=12)),
             sum((z>12) & (z<=20)), 
             sum((z>20)) )

# встроенный тест
chisq.test(x = chi_o, p = chi_c)

# посчитаем Хи-квадрат вручную, критическое значение K = 18.54 для p = 0.05 и выбранного количества интервалов. 
sum(((chi_o[1:8] - chi_c[1:8]*length(z))^2)/(chi_c[1:8]*length(z)))

```
В соответствии с критерием Пирсона наше модельное распределение явсляется нормальным. Критерий Хи-квадрат вычисленный вручную и вычисленный в рамках теста равны. 


Проведем еще два популярных теста: Шапиро-Уилка и Харке-Бера (используеющих асиммметрию и эксцесс):
```{r}
jarque.test(z)
shapiro.test(z)
```
Тесты уверенно говорят о том, что распределение нормальное. 

Теперь вместо реальных случайных значений подставим в нашу выборку ближайшее среднее по неравномерным интервалам. И снова проверим параметры выборки и проведем тесты. Понятно, что выбор интервалов усреденния тоже может играть роль, поэтому выберем интервалы, близкие к тем, что получаются на практике. 

```{r}
z[(z<=0.25)] <- mean(z[(z<=0.25)]) 
z[(z>0.25) & (z<=0.75)] <- mean(z[(z>0.25) & (z<=0.75)]) 
z[(z>0.75) & (z<=1.5)] <- mean(z[(z>0.75) & (z<=1.5)]) 
z[(z>1.5) & (z<=3)] <- mean(z[(z>1.75) & (z<=3)])
z[(z>3) & (z<=6)] <- mean(z[(z>3) & (z<=6)])
z[(z>6) & (z<=12)] <- mean(z[(z>6) & (z<=12)])
z[(z>12) & (z<=20)] <- mean(z[(z>12) & (z<=20)]) 
z[(z>20)] <- mean(z[(z>20)])

mean(z)
sd(z)
```
Среднее не изменилось, слегка уменьшилась дисперсия.

Проверим выборку по критерию Хи-квадрат. 
```{r}
# плотность распределения по модельным данным (не нормированная)
chi_o <- c(sum((z<=0.25)), 
             sum((z>0.25) & (z<=0.75)), 
             sum((z>0.75) & (z<=1.5)), 
             sum((z>1.5) & (z<=3)), 
             sum((z>3) & (z<=6)), 
             sum((z>6) & (z<=12)),
             sum((z>12) & (z<=20)), 
             sum((z>20)) )

# встроенный тест
chisq.test(x = chi_o, p = chi_c)

# посчитаем Хи-квадрат вручную, критическое значение K = 18.54 для p = 0.05 и выбранного количества интервалов. 
sum(((chi_o[1:8] - chi_c[1:8]*length(z))^2)/(chi_c[1:8]*length(z)))
```
Очень хорошо! Критерий Хи-квадрат в разы меньше критической точки (K = 18.5). P-значение большое 0.5.

Проверим асимметрию и экцесс.
```{r}
skewness(z)
kurtosis(z)
```
Асимметрий близка к нулю, а эксцесс немного уменьшился. 


А что нам покажут тесты Харке-Бера и Шапиро-Уилка?
```{r}
jarque.test(z)
shapiro.test(z)

```
Все хорошо! Распределение осталоcь нормальным (достоверность тестов уменьшилась, но несущетсвенно).


Для наглядности построим распределение.
```{r}
ggplot() + geom_histogram(aes(x = z), fill = "gray70", colour = "gray10") +
  scale_x_continuous(breaks=seq(-15,15,by=1)) +
  theme_bw()
```



# Основные параметры реальной выборки 

Итак, я взял все бажные тикеты, закрытые одной командой за последние 2.5 года в рамках одного большого проекта. Состав команды за это время сменился примерно на половину, но это происходило плавно. Костяк команды не менялся, в него входит 4 разработчика с опытом работы больше 15 лет (продуктовая команды, С++/С#). Общая численность команды колебалась от 8 до 11 человек включая тестировщика и дизайнера/UX-специалиста. 

Надо отметить, что оценка тикетов производится либо методом покет-планировая, либо методом экспертной оценки. При закрытии спринта оценка корректируется, если реальные трудозатраты отличались от плановых. В любом случае всегда единицы измерения - "идеальные" трудочасы. 

Полная выгрузка из Redminе включала около 1050 записей. После процедуры очистки осталось чуть большое 900. Вполне приличная выборка для статистического анализа. В ходе очистки из выборки были исключены тикеты без оценки или с 0-ой оценкой (такое бывает, если баг исчез вместе с модулем, или его исправление стало следствие рефакторинга, проведенного в рамках другой задачи и т.п.). Пара десятков тикетов имели "нестандартные" оценки (например, 5 часов), которые были приведены к ближайшему "стандартному" целому. Некоторые тикеты были "потеряны" - я привязал их к соответствующему спринту по дате закрытия. В выборку попали все баги, закрытые данной командой за последние 2.5 года. Это важно, иначе пришлось бы говорить об учете эффекта селекции. 

Загрузим очищенные данные и проверим итоговый размер выбоки: 
```{r}  
bugs <- readRDS("bugs.RDS")
nrow(bugs)
```

Посмотрим на распределение:
```{r}
ggplot(data = bugs) + geom_histogram(aes(x = hours), fill = "gray70", colour = "gray10") +
  scale_x_continuous(breaks=seq(0,41,by=1)) +
  theme_bw()
```

Ну, на первый неискушенный взгляд кажется, что гистограмма выглядит весьма "нормально", хотя и подрезано слева.


Проверим самые важные параметры. Итак... Момент Истины! Барабанная дробь.... 
```{r}  
median(bugs$hours)

bugs_m <-mean(bugs$hours)
bugs_m
```
Просто замечательно! Во-первых, медиана и среднее лежат очень близко друг к другу. А во вторых, предложение взять 4 часа в качестве значения по умолчанию (сделанное на глазок, без всякиз рассчетов) оказалось "не в бровь, а в глаз"! Лично я верю, что это интуиация очень опытного человека, а не самосбывающийся прогноз. 

А что с разбросом? 
```{r}  
bugs_sd <-sd(bugs$hours)
bugs_sd
```
Имеем среднеквадратическое отклонение около 4 часов. Мы пока не знаем, имеем ли мы дело с нормальным распределением или нет, но давайте опредлим границы по критерию 3-сигмы. 

```{r}
mean(bugs$hours) + sd(bugs$hours)*3
mean(bugs$hours) - sd(bugs$hours)*3
```
Вот уже в этом месте стоит призадуматься. Во-первых, совершенно очевидно, что работы с отрицательными трудозатратами не бывает. Соответственно наша выборка будет иметь большую асимметрию. Во-вторых, 16-ти и 24-х часовые тикеты оказываются за границами инетрвала в 3-сигмы. На тестовых данных мы видим, что 
```{r}
sum(z>=16)
sum(z>=14)
```
таких тикетов нет. Вероятность их появления в случае нормального распределения будет равна
```{r}
1 - pnorm(24, mean = bugs_m, sd = bugs_sd)
1 - pnorm(16, mean = bugs_m, sd = bugs_sd)
```
соответственно. Про 40-ка часовые и говорить нечего.... Однако мы знаем, что на практике это не так - такие задачи встречаются, причем регулярно. А что в реальной выборке?  
```{r}
sum(bugs$hours==16)
sum(bugs$hours==24)
sum(bugs$hours==40)
sum(bugs$hours>=16)
```
В реальной выборке мы видим, что такие тикеты есть и их относительно много.
Самое время найти ответ на второй вопрос (тем более, что он проще)...

# Является ли данное распределение нормальным? 

Проверим еще две характреристики, асимметрию (должна быть равна 0) и эксцесс (должен быть близок к 3). Асимметрия: 
```{r}
skewness(bugs$hours)
``` 
далека от нуля и положительна. Это говорит о том, что распределение "завалено" вправо. 
Эксцесс:
```{r}
kurtosis(bugs$hours)
```
что в несколько раз больше 3. Что в свою очередь говорит о том, что распределение "приподнято" по краям, "размазано". 
Таким образом вероятность "поймать" баг с трудозатратами более 16-ти часов больше, чем это было бы характерно для нормального распределения. 

Используем критерий Хи-квадрат.
```{r}
chi_o <- c(sum(bugs$hours==0.1), 
             sum(bugs$hours==0.5), 
             sum(bugs$hours==1), 
             sum(bugs$hours==2), 
             sum(bugs$hours==4), 
             sum(bugs$hours==8),
             sum(bugs$hours==16), 
             sum(bugs$hours>=24))

chisq.test(x = chi_o, p = chi_c)
```
Критерий Хи-квадрат просто огромен, а p-значение практически равно нулю, что говорит о том, что наша выборка не является нормально распределенной. 

Прогоним тесты Шапиро-Уилка и Харке-Бера. 
```{r}
shapiro.test(bugs$hours)
jarque.test(bugs$hours)
```
Тесты подтверждают наш вывод. Данное распределение никак нельзя считать нормальным. Соответственно, нельзя использовать свойства нормально распределения для оценки вероятности, нельзя использоваться критерий 3-сигмы, а в силу большой асимметрии надо отдельно разбираться с вопросом компенсации трудозатрат на практике. 


# Что же получается на практике? 

Пришло время понять, правильно ли выбрана оценка сложного бага по умолчанию в 4 часа и происходит ли компенсацция увеличения трудозатрат на сложные баги за счет экономии времени на простых багах. 

Для начала давайте поймем, какую доля нашей работы приходится на легкие и на сложные баги в целом. 
```{r}
# доля вклада багов разной сложности и вероятности появления багов разной сложности исходя из фактической частотности событий

total <- sum(bugs$hours)
#общее количество трудозатрат на баги за весь период
print(total)
bugs_sum <- matrix(0, ncol = 3, nrow = 4)
colnames(bugs_sum) <- c("t<4", "t=4", "t>4")
rownames(bugs_sum) <- c("N", "H (hours)", "N, %", "H, %")

# количество простых багов и трудозатраты на них
bugs_sum[1,1] <- sum(bugs$hours<4)
bugs_sum[2,1] <- sum(bugs$hours[bugs$hours<4])

# количество багов средней сложности и трудозатраты на них
bugs_sum[1,2] <- sum(bugs$hours==4)
bugs_sum[2,2] <- sum(bugs$hours[bugs$hours==4])

# количество багов высокой сложности и трудозатраты на них
bugs_sum[1,3] <- sum(bugs$hours>4)
bugs_sum[2,3] <- sum(bugs$hours[bugs$hours>4])

# доля багов каждой категории от общего количества в процентах
bugs_sum[3,] <- 100 * bugs_sum[1,]/nrow(bugs)
# доля трудозатрат на баги каждой категории от общих трудозатрат в процентах
bugs_sum[4,] <- 100 * bugs_sum[2,]/total

print(bugs_sum,digits = 3)

```
Получается довольно интересно: 50% трудозатрат порождает лишь 20% сложных багов. Еще 50% простых багов порождают лишь 15% трудозатрат. 

Но пока это нам ничего не дает. Посмотрим на ситуацию по другому. Сложные баги изначально чаще всего оценивались по умолчанию в 4 часа. А уже после исправления подросли. В результате на таких багах по факту мы получаем перепланирование 
$ dS_b = i*4 - S_b$, где 
   $S_b$ - фактические трудозатраты на подросшие баги, 
   $i$ - количество таких багов. 
Иными словами - взяли на спринт больше чем можем сделать, ситуация переполнения, и это плохо. И или подругому, получается перерасход часов на исправление сложных багов. 
Простые баги обычно сразу оцениваются менее чем в 4 часа. Если бы мы их тоже оценивали в 4 часа, то по факту имели бы недопланирование (или экономию, то есть на спринте оказывалось бы меньше часов, чем мы можем сделать, и это в общем-то хорошо). 
$ dS_s = j*4 - S_s$, где  
   $S_s$ - фактические трудозатраты на простые баги, 
   $i$ - количество простых багов. 
Общая сумма затрат $S_{total} = S_b + S_4 + S_s$, где $S_4 = k*4$ - сумма затрат на баги, оценка которых не изменилась и осталась прежней, 4 часа. А интерсует нас прежде всего величина $dS = dS_b + dS_s$, определяющая баланс или компенсацию. Отрицательная величина $dS$ говорит о том, что компенсация не происходит и на сложные баги времени уходит больше, чем мы могли бы экономить на простых. Дальнейшее рассмотрение основывается на этой концепции экономии на простых багах и перерасходе на сложных относительно средней оценки в 4 часа. 

Если бы все баги были априорно оценены в 4 часа, то:
```{r}
# фактически недоучтенное время на спринтах (перерасход на сложных багах) составило бы:
t1 <- bugs_sum[1,3]*4 - bugs_sum[2,3]
print(t1)

#  недопланированное время на спринтах (время, которое было бы сэкономлено, если бы исходная оценка была стандартной - 4 часа)
t2 <- bugs_sum[1,1]*4 - bugs_sum[2,1]
print(t2)

# баланс в виде доли от общего объема в процентах
print(100*abs(t1-t2)/(total), digits = 3)
```

**Важный промежуточный вывод: в среднем, на достаточно длинном интервале времени происходит компенсация потерь времени на сложных багах за счет экономии на простых**. Если бы мы всегда все баги априори оценивали в 4 часа, а после исправления корректировали бы оценку, мы бы все равно вышли бы примерно на туже итоговую сумму и, что более важно, мы имели бы баланс между сложными и простыми багами, то есть экономия на простых багах компенсировала бы перерасход времени на сложных. **И в этом случае априорная оценка в 4 часа выбрана правильно.** 

Одноко есть одно важное "но". Дело в том, что мы работаем недельными спринтами и на спринт обычно попадает небольшое количество багов. Соответственно, вопрос о том, происходит ли компенсация в рамках спринта остается открытам и требует дополнительного изучения. К счастью, у нас есть информация о том, на каком спринте какой баг был исправлен и мы может постараться найти ответ и на этот вопрос. 

# Гипотетическая компенсация в пределах спринта

Построим сводную таблицу по спринтам и посмотрим как по ним распределяются баги. 
```{r}

# сводная таблица по спринтам
stages <- bugs %>% group_by(sprint) %>% summarise(mean = mean(hours), median = median(hours), n = n(), total = sum(hours))

index <- seq(1,nrow(stages),1)
ggplot(data = stages) + theme_bw() + scale_x_continuous(breaks=seq(0,80,by=10)) +
  geom_line(aes(x = index, y = n))
  
```

Количество багов на спринте так же является случайной величиной и как мы видим колеблется достоточно сильно. На отдельных спринтах исправляется буквально 5-7 багов, но есть и такие, где было исправлено большо 20. 

В цифрах:
```{r}
# среднее, медиана и среднеквадратическое отклонение количества бажных тикетов на спринт
mean(stages$n)
median(stages$n)
sd(stages$n)

# среднее, медиана и среднеквадратическое отклонение общих трудозатрат на бажные тикетов на спринт
mean(stages$total)
median(stages$total)
sd(stages$total)
```
Получается, что в среднем мы имеем на спринте около 10 бажных тикетов примерно на 40 идеальных часов. Но разброс действительно весьма велик. Примерно для трети спринтов количество тикетов на спринте явно не достаточно, чтобы компенсация работала, но об этом ниже. 

```{r}
# проверим себя, среднее, медиана и среднеквадратическое отклонение средние трудозатрат на один баг, должны получить ~4 часа, 
mean(stages$mean)
median(stages$mean)
```
Сходится. 

Хорошо, а что с компенсация по спринтам? Развернем исходную таблицу по спринтам и для каждого спринта вычислим перерасход $dS_b^i$ (за счет недооценненых сложных багов, отрицательное число), экономию $dS_s^i$ (которая могла бы быть за счет простых багов, положительное число) и баланс $dS^i$ (он же компенсация = перерасход + экнономия). Построим распределение баланса по спринтам:
```{r}
sprints <- spread(bugs, sprint, hours)
sprints[is.na(sprints)] <- 0
sprints <- sprints[,-1]
sprints <- sprints[,-1]
sprints <- t(as.matrix(sprints))

sprints_s <- sprints
sprints_s[sprints_s>=4] <- 0
sprints_b <- sprints
sprints_b[sprints_b<=4] <- 0

sprints_stat <- cbind.data.frame(rowSums(sprints<4 & sprints>0), 
                                 rowSums(sprints==4), 
                                 rowSums(sprints>4), 
                                 rowSums(sprints>0),
                                 rowSums(sprints_s), 
                                 rowSums(sprints_b), 
                                 rowSums(sprints))
rownames(sprints_stat) <- rownames(sprints)
colnames(sprints_stat) <- c("small", "middle", "big", "all", "total_s", "total_b", "total")

# сэкономленное время
sprints_stat$t1 <- sprints_stat$small*4 - sprints_stat$total_s
# недопалнированное время
sprints_stat$t2 <- sprints_stat$big*4 - sprints_stat$total_b

sprints_stat$balance <-  sprints_stat$t1 + sprints_stat$t2

ggplot(data = sprints_stat) + geom_histogram(aes(x = balance), fill = "gray70", colour = "gray10") +
  scale_x_continuous(breaks=seq(-60,60,by=5)) +
  theme_bw()

```

Перерасход имеет отрицательный знак, экономия положительный. Уже по гистограмме видно, что распределение тоже не очень-то нормальное. И мы имеет очень большой разброс баланса на спринтах. Бывали спринты, где было много сложных тикетов (недооцененных), но было и довольно много спринтов, когда могла иметь место существенная экономия на простых тикетах. 


В цифрах:
```{r}
# суммарный баланс по всем спринтам относительно общего объема трудозатрат в процентах (контрольно), должны выйти на цифру близкую к 0, как мы это видели раньше
print(100*abs(sum(sprints_stat$balance))/total, digits = 3)

# средний баланс
print(mean(sprints_stat$balance), digits = 2)

# медианный баланс
print(median(sprints_stat$balance), digits = 3)

# среднеквадратическое отклонение
print(sd(sprints_stat$balance), digits = 3)

# количество спринтов, где баланс получился положительным или достаточно близким к нулю. 
100*sum(sprints_stat$balance>=-4)/nrow(sprints_stat)

```
Среднее -0.7, медиана 2.5, а мода судя по гистограмме >5. Среднеквадратическое отклонение очень большое - 20. 
И еще один важный промежуточный результат: **однако более чем на 2/3 спринтов баланс получился близким нулю или положительным!**  

Построим график перерасхода $dS_b$ (красный), экономии $dS_s$(зеленый) и балланса $dS$ (черый) по спринтам. 
```{r}
sprints_stat$name <- rownames(sprints_stat)
sprints_stat <- arrange(sprints_stat, name)


ggplot(data = sprints_stat) + theme_bw() + scale_x_continuous(breaks=seq(0,80,by=10)) +
# сэкономленное время  
  geom_line(aes(x = index, y = sprints_stat$t1), colour = "green", size = 0.5) +  
# недопланированное время  
  geom_line(aes(x = index, y = sprints_stat$t2), colour = "red", size = 0.5) +
#баланс
  geom_line(aes(x = index, y = sprints_stat$balance), size = 1)
```

Хорошо видно, что ситуация несколько улучшилась где-то в районе 35-го спринта (переход на недельные спринты?). И в последнее время мы чаще могли бы иметь положительный баланс, чем отрицательный. 

**Таким образом, априорная оценка в 4 часа видится верной в случае, если она назначается всем тикетам на спринте, и на спринт берется достаточно большое количество бажных тикетов. Однако На практике простые баги априори получают низкие оценки трудозатрат и реальной экономии на простых багах практически нет. ** Наш фактичесий баланс близок к красной линии. Средний перерасход по спринтам: 
```{r}
mean(sprints_stat$t2[sprints_stat$t2<0])
```
**В этом случае априорная оценка сложных багов в 4 часа видится заниженной.**

А сколько бажных тикетов мы должны иметь на спринте для того, чтобы компенсация устойчиво работала (допустим, мы всем бажным тикетам назначем априорную оценку равную среднему, то есть 4 часа)? 
(In progress...)


# Выводы 

1. Распределение трудозатрат на исправление ошибок в ПО не является нормальным. По крайней мере в данном конкретном случае. Оно имеет большую положительную асимметрию и просто колоссальный эксцесс. Поэтому оценка по методике 3-сигмы по формальным параметрам выборки будет давать неверные, сильно заниженные вероятности справа и ее нельзя применять.

Рассчитаем вероятность появления "черного лебедя" двумя способами: а) в рамках теоретического нормального распределения и б) опираясь на частоты в реальной выборке. 
```{r}
# "black swan" probability (out of 3-sigma ) in percents in within normal distribution 
(1-pnorm(mean(bugs$hours) + 3*sd(bugs$hours), mean = mean(bugs$hours), sd = sd(bugs$hours)))*100

# the same within real sample:
sum(bugs$hours>=16)/nrow(bugs)*100
```
Таким образом, **вероятность "поймать черного лебедя" в реальности в 25 раз больше, чем если бы имели дело с нормальным распределением.** 

Сравним плотности вероятности с точки зрения правила трех сигм справа. 
```{r}
# 1-сигма
quantile(bugs$hours, probs = 0.683)
qnorm(0.683, mean = bugs_m, sd = bugs_sd)

# 2-сигмы
quantile(bugs$hours, probs = 0.954)
qnorm(0.954, mean = bugs_m, sd = bugs_sd)

# 3-сигмы
quantile(bugs$hours, probs = 0.997)
qnorm(0.997, mean = bugs_m, sd = bugs_sd)
```
Как можно видеть, вероятность попадания бага в 1-ю и 2-ю сигму не слишком отличаются, зато вероятность попадания в третью сигму на реальной выборке на много выше. 


2.1 Баги с отрицательным временем исправления отсутвуют по определению (достаточно большая асимметрия). Но компесация (редукция к среднему) в среднем на большой выборке должна происходить. На практике объем "сэкономленного времени" на простых багах (априори переоцениваемых) оказывается сопостовимым с объемом потерянного времени на сложных багах (априори недооцениваемых). 

2.2 Мы работаем недельными спринтами и нас прежде всего интересует точность планирования, точнее возможность выполнить все задачи, взятые на спринт (добавить что-то еще мы всегда можем, ситуация когда мы что-то не успеваем значительно хуже). Компенсанция в рамках спринта происходила бы, если бы а) мы всегда для всех тикетов назначали исходную оценку в 4 часа и б) на спринт каждый раз попадало бы достаточное число тикетов. Однако на практике первое условие не выполняется в силу применяемой тактики априорной оценки тикетов, а второе условие выполяется в лучшем случае в половине случаев (смотря какой критерий использовать). 

2.2.1 Если баг простой и понятный то мы очень часто сразу прямо на планировании назначаем меньшую оценку (1, 2 часа). Но редко назначаем сложному багу большую чем 4 часа оценку. При этом если на баг уходит больше 4-х часов и это не "черный лебедь" он исправляется прямо на спинте, получая итоговую оценку в 8-24 часа. 

2.2.2 Среднее количество багов, исправляемых за спринт около 10, но разброс большой. В то же время спринтов, на которых было исправлено менее 5-ти багов всего 6.  

2.3 Таким образом на практике компенсация скорее всего происходит значительно реже, чем это могло бы быть в том случае, если бы на спринты бралось достаточно много багов и всем багам назначалась бы оценка равная среднему (4 часа). К сожалению, у нас есть только окончательная оценка тикета после исправления, и нет априорных, поэтому на данный момент невозможно точно вычислить фактический эффект компенсации.  

3.1 **Надо отметить, что настоящие "черные лебеди" в эту статистику скорее всего вообще не попадают.** Речь идет о действительно сложных ситуациях, когда после длительных исследований и многократных попыток разобраться мы таки выясняем в чем дело и получаем несколько тикетов, в том числе с рефакторингом и доработками. При этом потери на исследования могут вообще не учитываться. 

3.2 Бывает и так, что один и тот же баг исправляется несколько раз (неудачно) и воскресает спустя какое-то время как птица Феникс. Правильнее было бы суммировить трудозатраты по всем попыткам, но сейчас в статистику попадает несколько небольших исправлений, а не одно большое. Поэтому правый край распределения багов по трудозатратам должен быть еще выше. 

Так что делать чтобы улучшить планирование? 

Вариант 1. Оставить текущую тактику априорной оценки, но для сложных тикетов использовать завышенную априорную оценку, например 6 или 8 часов. В частных случаях пытаться по возможности использовать и другие, бОльшие оценки, если к этому есть предпосылки. 

Вариант 2. Использовать для всех багов оценку 4 часа, но при этом брать на спринт достаточно количество багов, чтобы компенсация работала. 


# Заключение

Таким образом, краткие ответы на вопросы, поставленные вначале будут такими: 

1. Исследуемое распределение не является нормальным;

2. Вероятность поймать действительно сложный баг в жизни значительно больше, чем это предсказывает нормальное распределение;

3. Однако в среднем на длительном интервале времени компенсация увеличения трудозатрат на сложные баги за счет экономии на простых происходит;

4. Но только в рамках всей выборки, а в рамках рабочих спринтов ситуация хуже, на баланс рассчитывать не приходиться: примерно в трети спринтов гипотетический баланс существенно отрицателый;

5. В жизни ситуацию еще более осложняет текущая методика априорной оценки, которая практически исключает эффект компенсации; средняя оценка недопланирования на спринте составляет $-16\pm 14$ часов;

6. Черные лебеди конечно существуют, но как правило не попадают в статистику (аналог игровой ошибки по Талебу), так как в процессе "переваривания" раскладываются на множество тикетов, "размазанных" по нескольким спринтам, что искажает нашу оценку в лучшую сторону. 





